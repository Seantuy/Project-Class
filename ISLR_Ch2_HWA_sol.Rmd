---
title: 'ISLR - Chapter 2: Statistical Learning'
author: "Solutions to Exercises 6, 8--10"
date: "`r format(Sys.time(), '%a %d %b %Y')`"
output:
  pdf_document: default
  html_document:
    keep_md: no
---

***
## Conceptual Exercises
***

<a id="ex06"></a>

>EXERCISE 6: Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disavantages?

Answer: 

For parametric methods, we make an assumption about the shape of the underlying data, select a model form, and fit the data to our selected form. The advantage here is that we can incorporate any prior/expert knowledge and don't tend to have too many parameters that need to be fit. To the extent that our prior/expert assumptions are wrong, then that would be a disadvantage.

Non-parametric methods don't make explicit assumptions on the shape of the data. This can have the advantage of not needing to make an assumption on the form of the function and can more accurately fit a wider range of shapes for the underlying data. The key disadvantage is that they need a large number of observations to fit an accurate estimate.

***

<a id="ex07"></a>


***
## APPLIED
***


>EXERCISE 8:

__Part a)__

```{r, warning=FALSE, message=FALSE}
require(ISLR)
data(College)
str(College)
```

__Part b)__

```{r, eval=FALSE}
# these steps were already taken on College data in the ISLR package
fix(College)  # pops up table in window
rownames(College) <- College[,1]  # set row names
College <- College[,-1]  # drop first col
```

__Part c)__

```{r}
# i.
summary(College)
# ii.
pairs(College[,1:10])
# iii.
boxplot(Outstate~Private, data=College, xlab="Private", ylab="Outstate")
# iv.
Elite <- rep("No", nrow(College))
Elite[College$Top10perc>50] <- "Yes"
College <- data.frame(College, Elite)
summary(College)  # 78 Elite
boxplot(Outstate~Elite, data=College, xlab="Elite", ylab="Outstate")
# v. 
par(mfrow=c(2,2))
hist(College$Apps, breaks=50, xlim=c(0,25000), main="Apps")
hist(College$Enroll, breaks=25, main="Enroll")
hist(College$Expend, breaks=25, main="Expend")
hist(College$Outstate, main="Outstate")
# vi.

```

***

<a id="ex09"></a>

>EXERCISE 9:

__Part a)__

```{r, warning=FALSE, message=FALSE}
require(ISLR)
data(Auto)
str(Auto)
```

* quantitative: _mpg, cylinders (can treat as qual too), displacement, horsepower, weight, acceleration, year_
* qualitative: _origin, name_

__Part b)__

```{r}
range(Auto$mpg)
range(Auto$cylinders)
range(Auto$displacement)
range(Auto$horsepower)
range(Auto$weight)
range(Auto$acceleration)
range(Auto$year)
```

__Part c)__

```{r}
sapply(Auto[,1:7], mean)
sapply(Auto[,1:7], sd)
```

__Part d)__

```{r}
# create temp matrix for numeric columns
tmp <- Auto[,-(8:9)]   # drop origin, name
tmp <- tmp[-(10:85),]  # drop rows
sapply(tmp, range)
sapply(tmp, mean)
sapply(tmp, sd)
```

__Part e)__

```{r}
pairs(Auto[,1:7])
```

* `mpg` is negatively correlated with `cylinders`, `displacement`, `horsepower`, and `weight`
* `horsepower` is negatively correlated with `weight`
* `mpg` mostly increases for newer model years

__Part f)__

yes, the plots show that there are relationships between `mpg` and other variables in the data set

***

<a id="ex10"></a>

>EXERCISE 10:

__Part a)__

```{r, warning=FALSE, message=FALSE}
require(ISLR)
require(MASS)
data(Boston)
str(Boston)  # 506 rows, 14 cols
```

* data set includes 506 rows and 14 columns
    * rows represent observations for each town 
    * columns represent features

__Part b)__

```{r}
pairs(Boston)
```

relationship between crime rate per capita and other variables don't seem to be linear

__Part c)__

```{r, warning=FALSE, message=FALSE}
require(ggplot2)
require(reshape2)

# plot each feature against crim rate
bosmelt <- melt(Boston, id="crim")
ggplot(bosmelt, aes(x=value, y=crim)) +
  facet_wrap(~variable, scales="free") + 
  geom_point()
(corrmatrix <- cor(Boston, use="complete.obs")[1,])
corrmatrix[corrmatrix > 0.5 | corrmatrix < -0.5][-1]
```

* some correlations with each variable, except chas
* crim rates seem to spike within certain zones
    * when `rad` is > 20
    * when `tax` is between 600 and 700
    * when `zn` is close to 0
    * etc.
* negative correlations with `dis`, `medv` and maybe `black`

__Part d)__

```{r}
require(ggplot2)
g <- ggplot(Boston, aes(x=1:nrow(Boston), y=crim))
g + geom_point()
g <- ggplot(Boston, aes(x=1:nrow(Boston), y=tax))
g + geom_point()
g <- ggplot(Boston, aes(x=1:nrow(Boston), y=ptratio))
g + geom_point()
```

* definitely outliers for `crim` and `tax`
* no clear outlier for `ptratio`

__Part e)__

```{r}
table(Boston$chas)  # 35 towns
```

__Part f)__

```{r}
median(Boston$ptratio)  # 19.05
```


__Part g)__

```{r}
# there are two towns with lowest medv value of 5
(seltown <- Boston[Boston$medv == min(Boston$medv),])
# overall quartiles and range of predictors
sapply(Boston, quantile)
```

* `age`, `rad` at max
* `crim`, `indus`, `nox`, `tax`, `ptratio`, `lstat` at or above 75th percentile
* low for `zn`, `rm`, `dis`

__Part h)__

```{r}
# count of towns
nrow(Boston[Boston$rm > 7,])  # 64 with > 7 rooms
nrow(Boston[Boston$rm > 8,])  # 13 with > 8 rooms

# row 1: mean for towns with > 8 rooms per dwelling
# row 2: median for all towns
rbind(sapply(Boston[Boston$rm > 8,], mean), sapply(Boston, median))
```

* crim rates are higher (almost 3X)
* higher proportion of 25K sq ft lots
* much lower `lstat` value
* higher `medv` value
