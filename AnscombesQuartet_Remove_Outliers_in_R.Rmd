---
title: "Anscombe’s Quartet of ‘Identical’ Simple Linear Regressions"
author: 'MANOS Thanos'
date: "`r format(Sys.time(), '%a %d %b %Y')`"
output:
  pdf_document:
        latex_engine: xelatex
---


Visualization may not be as precise as statistics, but it provides a unique view onto data that can make it much easier to discover interesting structures than numerical methods. Visualization also provides the context necessary to make better choices and to be more careful when fitting models. Anscombe’s Quartet is a case in point, showing that four datasets that have identical statistical properties can indeed be very different.

Arguing for Graphics in 1973

In 1973, Francis J. Anscombe published a paper titled, Graphs in Statistical Analysis. The idea of using graphical methods had been established relatively recently by John Tukey, but there was evidently still a lot of skepticism. Anscombe first lists some notions that textbooks were “indoctrinating” people with, like the idea that “numerical calculations are exact, but graphs are rough.”

He then presents a table of numbers. It contains four distinct datasets (hence the name Anscombe’s Quartet), each with statistical properties that are essentially identical: the mean of the x values is 9.0, mean of y values is 7.5, they all have nearly identical variances, correlations, and regression lines (to at least two decimal places).

```{r, echo=T, eval=T}
knitr::kable(anscombe)
```

Let’s do the simple descriptive statistics on each data set

Here is mean of x and y

```{r, echo=T, eval=T}
anscombe.1 <- data.frame(x = anscombe[["x1"]], y = anscombe[["y1"]], Set = "Anscombe Set 1")
anscombe.2 <- data.frame(x = anscombe[["x2"]], y = anscombe[["y2"]], Set = "Anscombe Set 2")
anscombe.3 <- data.frame(x = anscombe[["x3"]], y = anscombe[["y3"]], Set = "Anscombe Set 3")
anscombe.4 <- data.frame(x = anscombe[["x4"]], y = anscombe[["y4"]], Set = "Anscombe Set 4")

anscombe.data <- rbind(anscombe.1, anscombe.2, anscombe.3, anscombe.4)
aggregate(cbind(x, y) ~ Set, anscombe.data, mean)
```

And SD

```{r, echo=T, eval=T}
aggregate(cbind(x, y) ~ Set, anscombe.data, sd)
```

And correlation between x and y

```{r, echo=T, eval=T}
library(plyr)

correlation <- function(data) {
    x <- data.frame(r = cor(data$x, data$y))
    return(x)
}

ddply(.data = anscombe.data, .variables = "Set", .fun = correlation)
```

As can be seen they are pretty much the same for every data set.

## Let’s perform linear regression model for each

```{r, echo=T, eval=T}
model1 <- lm(y ~ x, subset(anscombe.data, Set == "Anscombe Set 1"))
model2 <- lm(y ~ x, subset(anscombe.data, Set == "Anscombe Set 2"))
model3 <- lm(y ~ x, subset(anscombe.data, Set == "Anscombe Set 3"))
model4 <- lm(y ~ x, subset(anscombe.data, Set == "Anscombe Set 4"))
```


```{r, echo=T, eval=T}
summary(model1)
```

```{r, echo=T, eval=T}
summary(model2)
```

```{r, echo=T, eval=T}
summary(model3)
```

```{r, echo=T, eval=T}
summary(model4)
```

Now, do what you should have done in the first place: PLOTS


```{r, echo=T, eval=T}
library(ggplot2)

gg <- ggplot(anscombe.data, aes(x = x, y = y))
gg <- gg + geom_point(color = "black")
gg <- gg + facet_wrap(~Set, ncol = 2)
gg <- gg + geom_smooth(formula = y ~ x, method = "lm", se = FALSE, data = anscombe.data)
gg
```

While dataset I appears like many well-behaved datasets that have clean and well-fitting linear models, the others are not served nearly as well. Dataset II does not have a linear correlation; dataset III does, but the linear regression is thrown off by an outlier. It would be easy to fit a correct linear model, if only the outlier were spotted and removed before doing so. Dataset IV, finally, does not fit any kind of linear model, but the single outlier makes keeps the alarm from going off.

How do you find out which model can be applied? Anscombe’s answer is to use graphs: looking at the data immediately reveals a lot of the structure, and makes the analyst aware of “pathological” cases like dataset IV. Computers are not limited to running numerical models, either.

**A computer should make both calculations and graphs. Both sorts of output should be studied; each will contribute to understanding.**

What is an Outlier?

In addition to showing how useful a clear look onto data can be, Anscombe also raises an interesting question: what, exactly, is an outlier? He describes a study on education, where he studied per-capita expenditures for public schools in the 50 U.S. states and the District of Columbia. Alaska is a bit of an outlier, so it moves the regression line away from the mainstream. The obvious response would be to remove Alaska from the data before computing the regression. But then, another state will be an outlier. Where do you stop?

**Anscombe argues that the correct answer is to show both the regression with Alaska, but also how much it contributes and what happens when it is removed.**

The tool here, again, are graphical representations. Not only the actual data needs to be shown, but also the distances from the regression line (the residuals), and other statistics that help judge how well the model fits. It seems like an obvious thing to do, but presumably was not the norm in the 1970s, and I can imagine that it still not always is.

It can be seen both graphically and from regression summary that each data set resulted in same statistical model!

Intercepts, coeficients and their p values are the same. SEE (standard error of the estimate, or SD of residuals), F-value and it’s p values are the same.

**Conclusion: ALWAYS plot your data! And always do model diagnostics by plotting the residuals.**


```{r, echo=T, eval=T}
par(mfrow = c(2, 2))
plot(model1, main = "Model 1")
```


```{r, echo=T, eval=T}
par(mfrow = c(2, 2))
plot(model2, main = "Model 2")
```


```{r, echo=T, eval=T}
par(mfrow = c(2, 2))
plot(model3, main = "Model 3")
```


```{r, echo=T, eval=T}
par(mfrow = c(2, 2))
plot(model4, main = "Model 4")
```

**Bonus - How to remove outliers (there a few methods in R (1 variable)**

1. Define a function to do it (not straight-forward)
```{r, echo=T, eval=T}
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}

set.seed(1)
x <- rnorm(100)
x <- c(-10, x, 10)
y <- remove_outliers(x)

par(mfrow = c(1, 2))
boxplot(x)
boxplot(y)
```

2. Use built-in R function (easier method): 
```{r, echo=T, eval=T}
set.seed(1)
x <- rnorm(100)
x <- c(-10, x, 10)
y <- x[!x %in% boxplot.stats(x)$out]

par(mfrow = c(1, 2))
boxplot(x)
boxplot(y)
```








