---
title: "R Notebook"
output: html_notebook
---

Exercice 1 Read the file with the command :

```{r}
heart_df<- read.csv("H:/Downloads/heart_tidy.csv",sep=',', header = FALSE)
```

```{r}
heart_df$V2<-as.factor(heart_df$V2)
heart_df$V3<-as.factor(heart_df$V3)
heart_df$V6<-as.factor(heart_df$V6)
heart_df$V7<-as.factor(heart_df$V7)
heart_df$V9<-as.factor(heart_df$V9)
heart_df$V13<-as.factor(heart_df$V13)
heart_df$V14<-as.factor(heart_df$V14)
```


```{r}
head(heart_df)
```


Exercice 2 Split the data set into two subsets : a training set containing 70%
of the data and a test set containing the rest.

```{r}
set.seed(1234)
ind <- sample(2, nrow(heart_df), replace=TRUE, prob=c(0.7, 0.3))
trainData <- heart_df[ind==1,]
testData <- heart_df[ind==2,]
trainData
testData
```
Exercice 3 Construct a decision tree form the training set with the control
parameter minsplit = 10. Give the set of rules that allow to predict the Presence
of Heart Disease

```{r}
myFormula <- V14 ~ V1 + V2 + V3 + V4 + V5 + V6 +V7 +V8 + V9 + V10 +V11 +V12 +V13
library(rpart)
heart_rpart <- rpart(myFormula, data=trainData,
control = rpart.control(minsplit = 10))
attributes(heart_rpart)
print(heart_rpart)

library(rpart.plot)
prp(heart_rpart,extra=1)

trainPred <- predict(heart_rpart, newdata = trainData, type="class")
table(trainPred, trainData$V14)
testPred <- predict(heart_rpart, newdata = testData, type="class")
table(testPred, testData$V14)
```

Exercice 4 Give the error on the test set. Give the precision and the recall
for the class value Presence of Heart Disease

```{r}
heart_rpart$control$cp
plotcp(heart_rpart)
heart_rpart <- rpart(myFormula, data=trainData,
control = rpart.control(minsplit = 10, cp=0.2))
```
Exercice 5 Adjust a Random forest using the default settings. Find next an
"optimal" number of trees. Comment. Find a subset of the most important
variables and Comment.

```{r}
library(randomForest)
rf <- randomForest(V14 ~ ., data=trainData, ntree=100)
print(rf)
attributes(rf)
plot(rf$err.rate[,1], type="l",)
importance(rf)
varImpPlot(rf)
```
the most important variable is V8 V3 V12  V10 V13


Exercice 6 Give the error on the test set. Give the precision and the recall
for the class value Presence of Heart Disease. Compare with the decision tree.
Comment the results

```{r}

evaluator <- function(model,test,n) evaluator <- function(model, test, n) {
  # Predictions used for the confusion matrix   
  predictions <- predict(model, test)
  
  # Confusion Matrix   
  cm <- table(predictions, test[, n])
  
  # Calculate Precision (True Positives / (True Positives + False Positives))  
  precision <- diag(cm) / rowSums(cm)
  
  # Calculate Recall (True Positives / (True Positives + False Negatives))  
  recall <- diag(cm) / colSums(cm)
  
  # Calculate Accuracy (Sum of True Positives / Total Instances)  
  accuracy <- sum(diag(cm)) / sum(cm)
  
  # Calculate Error Rate (1 - Accuracy)
  errorRate <- 1 - accuracy
  
  
  print(precision)
}


```


Exercice 7 Construct a naif bayesian model and compare its performances to
both precedent methods.

```{r}
library(e1071)
model=naiveBayes(V14 ~ .,data=trainData)
attributes(model)
model$apriori
```

```{r}
trainPredBayes <- predict(model, newdata = trainData)
trainPredBayes
table(trainPredBayes, trainData$V14)
testPredBayes <-predict(model,newdata=testData)
table(testPredBayes, testData$V14)
```
Exercice 8 Observe the output tables and choose two variables having two
different types. Explain the table values corresponding to each of these variables.

```{r}
table(trainPredBayes, trainData$V14)
table(testPredBayes, testData$V14)

```

Exercice 9 We would like now to apply the K-means algorithm on only the 6
most important continuous variables selected by the RandomForest method. To
construct a data frame containing given columns extracted from another one,
you can do as follow :


newDF<-data.frame(oldDF$colName1, .., oldDF$colNamek, ..)


Find the best value of the parameter K and visualise found clusters in a twodimensional space (you choose two variables). Give the intra-cluster inertia.
```{r}
newDF<- data.frame(heart_df$V13,heart_df$V12,heart_df$V10,heart_df$V8,heart_df$V3,heart_df$V1)
```
13 12 10 8 3 1
```{r}
head(newDF)
```

```{r}
newDF2 <- newDF
newDF2$V14 <- NULL
(kmeans.result <- kmeans(newDF2, 7))
plot(newDF2[c("heart_df.V12", "heart_df.V10")], col = kmeans.result$cluster)
# plot cluster centers
points(kmeans.result$centers[,c("heart_df.V12", "heart_df.V10")],
col = 1:3, pch = 8, cex=2)
ratio_ss <- data.frame(cluster = seq(from = 1, to = 9, by = 1))
```

Exercice 10 Find the purest and the least pure clusters and compute the
purity of each of them.

```{r}

```







Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
